{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "build_platform": {
   "linux_aarch64": "linux_64",
   "linux_ppc64le": "linux_64",
   "osx_arm64": "osx_64"
  },
  "conda_forge_output_validation": true,
  "github": {
   "branch_name": "main",
   "tooling_branch_name": "main"
  },
  "test": "native_and_emulated"
 },
 "feedstock_name": "tokenizers",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": [
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1"
   ],
   "number": "1",
   "script": [
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.15.0"
  },
  "requirements": {
   "build": [
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ],
   "host": [
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl"
   ],
   "run": [
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\""
   ],
   "imports": [
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ],
   "requires": [
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6"
   ],
   "source_files": [
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "maturin",
    "openssl",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "huggingface_hub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl",
    "datasets",
    "dill",
    "numpy",
    "pip",
    "pytest",
    "requests"
   ]
  }
 },
 "linux_aarch64_meta_yaml": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": [
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1"
   ],
   "number": "1",
   "script": [
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.15.0"
  },
  "requirements": {
   "build": [
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ],
   "host": [
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl"
   ],
   "run": [
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\""
   ],
   "imports": [
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ],
   "requires": [
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6"
   ],
   "source_files": [
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests"
   ]
  }
 },
 "linux_aarch64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "maturin",
    "openssl",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "huggingface_hub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl",
    "datasets",
    "dill",
    "numpy",
    "pip",
    "pytest",
    "requests"
   ]
  }
 },
 "linux_ppc64le_meta_yaml": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": [
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1",
    "/usr/lib64/libgcc_s.so.1"
   ],
   "number": "1",
   "script": [
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.15.0"
  },
  "requirements": {
   "build": [
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ],
   "host": [
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl"
   ],
   "run": [
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\""
   ],
   "imports": [
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ],
   "requires": [
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6"
   ],
   "source_files": [
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests"
   ]
  }
 },
 "linux_ppc64le_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "maturin",
    "openssl",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "huggingface_hub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl",
    "datasets",
    "dill",
    "numpy",
    "pip",
    "pytest",
    "requests"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": null,
   "number": "1",
   "script": [
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.15.0"
  },
  "requirements": {
   "build": [
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ],
   "host": [
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "openssl",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0"
   ],
   "run": [
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0"
   ]
  },
  "source": {
   "patches": [
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch"
   ],
   "sha256": "7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\""
   ],
   "imports": [
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ],
   "requires": [
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6"
   ],
   "source_files": [
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests"
   ]
  }
 },
 "name": "tokenizers",
 "osx_64_meta_yaml": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": [
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib"
   ],
   "number": "1",
   "script": [
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.15.0"
  },
  "requirements": {
   "build": [
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ],
   "host": [
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0"
   ],
   "run": [
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\""
   ],
   "imports": [
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ],
   "requires": [
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6"
   ],
   "source_files": [
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests"
   ]
  }
 },
 "osx_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "maturin",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "huggingface_hub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl",
    "datasets",
    "dill",
    "numpy",
    "pip",
    "pytest",
    "requests"
   ]
  }
 },
 "osx_arm64_meta_yaml": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": [
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib",
    "/usr/lib/libresolv.9.dylib"
   ],
   "number": "1",
   "script": [
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.15.0"
  },
  "requirements": {
   "build": [
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ],
   "host": [
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0"
   ],
   "run": [
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0"
   ]
  },
  "source": {
   "patches": null,
   "sha256": "7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\""
   ],
   "imports": [
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ],
   "requires": [
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6"
   ],
   "source_files": [
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests"
   ]
  }
 },
 "osx_arm64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "maturin",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "huggingface_hub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl",
    "datasets",
    "dill",
    "numpy",
    "pip",
    "pytest",
    "requests"
   ]
  }
 },
 "outputs_names": {
  "__set__": true,
  "elements": [
   "tokenizers"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64",
  "linux_aarch64",
  "linux_ppc64le",
  "osx_64",
  "osx_arm64",
  "win_64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/tokenizers.json"
 },
 "raw_meta_yaml": "{% set tests_to_skip = \"_not_a_real_test\" %}\n{% set version = \"0.15.0\" %}\n\npackage:\n  name: tokenizers\n  version: {{ version }}\n\nsource:\n  url: https://github.com/huggingface/tokenizers/archive/refs/tags/v{{ version }}.tar.gz\n  sha256: 7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4\n  patches:\n    - patches/0001-don-t-fork-on-windows.patch  # [win]\n\nbuild:\n  number: 1\n  missing_dso_whitelist:\n    - /usr/lib/libresolv.9.dylib  # [osx]\n    - /usr/lib64/libgcc_s.so.1  # [linux]\n  script:\n    {% if build_platform != target_platform %}\n    - export PYO3_CROSS_INCLUDE_DIR=$PREFIX/include\n    - export PYO3_CROSS_LIB_DIR=$SP_DIR/../\n    - export PYO3_CROSS_PYTHON_VERSION=$PY_VER\n    # see below for what OPENSSL_DIR should be pointing to:\n    # https://github.com/sfackler/rust-openssl/blob/openssl-sys-v0.9.72/openssl/src/lib.rs#L55-L56\n    - export OPENSSL_DIR=$PREFIX\n    {% endif %}\n    - cd bindings/python\n    - {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  build:\n    - python                                 # [build_platform != target_platform]\n    - cross-python_{{ target_platform }}     # [build_platform != target_platform]\n    - openssl                                # [build_platform != target_platform]\n    - maturin >=1.0,<2.0                     # [build_platform != target_platform]\n    - {{ compiler('cxx') }}\n    - {{ compiler('rust') }}\n  host:\n    - python\n    - pip\n    - maturin >=1.0,<2.0\n    - openssl    # [linux]\n  run:\n    - python\n    - huggingface_hub >=0.16.4,<1.0\n\ntest:\n  imports:\n    - tokenizers\n    - tokenizers.models\n    - tokenizers.decoders\n    - tokenizers.normalizers\n    - tokenizers.pre_tokenizers\n    - tokenizers.processors\n    - tokenizers.trainers\n    - tokenizers.implementations\n  requires:\n    - pip\n    - pytest\n    - datasets\n    - numpy *\n    - requests\n    - curl *\n    # temp: fix until https://github.com/conda-forge/multiprocess-feedstock/pull/46\n    # percolates far enough so that the solver doesn't pull in an old version anymore\n    - dill >=0.3.6\n  source_files:\n    - bindings/python/tests\n  commands:\n    - pip check\n    # upstream requires running the tests from this directory\n    - cd bindings/python\n    # adapted from https://github.com/huggingface/tokenizers/blob/master/bindings/python/Makefile\n    - mkdir data\n    - curl https://norvig.com/big.txt > data/big.txt\n    {% set tests_to_skip = \"_not_a_real_test\" %}\n    # windows and expectation of forking -> not gonna happen\n    {% set tests_to_skip = tests_to_skip + \" or with_parallelism\" %}  # [win]\n    - pytest -v tests -k \"not ({{ tests_to_skip }})\"\n\nabout:\n  home: https://pypi.org/project/tokenizers/\n  license: Apache-2.0\n  license_family: APACHE\n  license_file: LICENSE\n  summary: Fast State-of-the-Art Tokenizers optimized for Research and Production\n  dev_url: https://github.com/huggingface/tokenizers\n\nextra:\n  recipe-maintainers:\n    - anthchirp\n    - ndmaxar\n    - oblute\n    - setu4993\n    - h-vetinari\n    - xhochy\n",
 "req": {
  "__set__": true,
  "elements": [
   "cxx_compiler_stub",
   "huggingface_hub",
   "maturin",
   "openssl",
   "pip",
   "python",
   "rust_compiler_stub"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "maturin",
    "openssl",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "huggingface_hub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl",
    "datasets",
    "dill",
    "numpy",
    "pip",
    "pytest",
    "requests"
   ]
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "maturin >=1.0,<2.0",
    "openssl",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "huggingface_hub >=0.16.4,<1.0",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl *",
    "datasets",
    "dill >=0.3.6",
    "numpy *",
    "pip",
    "pytest",
    "requests"
   ]
  }
 },
 "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz",
 "version": "0.15.0",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/tokenizers.json"
 },
 "win_64_meta_yaml": {
  "about": {
   "dev_url": "https://github.com/huggingface/tokenizers",
   "home": "https://pypi.org/project/tokenizers/",
   "license": "Apache-2.0",
   "license_family": "APACHE",
   "license_file": "LICENSE",
   "summary": "Fast State-of-the-Art Tokenizers optimized for Research and Production"
  },
  "build": {
   "missing_dso_whitelist": null,
   "number": "1",
   "script": [
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv",
    "cd bindings/python",
    "PYTHON -m pip install . -vv"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy",
    "anthchirp",
    "ndmaxar",
    "oblute",
    "setu4993",
    "h-vetinari",
    "xhochy"
   ]
  },
  "package": {
   "name": "tokenizers",
   "version": "0.15.0"
  },
  "requirements": {
   "build": [
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub",
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ],
   "host": [
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0",
    "python",
    "pip",
    "maturin >=1.0,<2.0"
   ],
   "run": [
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0",
    "python",
    "huggingface_hub >=0.16.4,<1.0"
   ]
  },
  "source": {
   "patches": [
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch",
    "patches/0001-don-t-fork-on-windows.patch"
   ],
   "sha256": "7f749421dc97e143cd426816f253923a562b235b187b4bd345765af9f86b11e4",
   "url": "https://github.com/huggingface/tokenizers/archive/refs/tags/v0.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\"",
    "pip check",
    "cd bindings/python",
    "mkdir data",
    "curl https://norvig.com/big.txt > data/big.txt",
    "pytest -v tests -k \"not (_not_a_real_test or with_parallelism)\""
   ],
   "imports": [
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations",
    "tokenizers",
    "tokenizers.models",
    "tokenizers.decoders",
    "tokenizers.normalizers",
    "tokenizers.pre_tokenizers",
    "tokenizers.processors",
    "tokenizers.trainers",
    "tokenizers.implementations"
   ],
   "requires": [
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6",
    "pip",
    "pytest",
    "datasets",
    "numpy *",
    "requests",
    "curl *",
    "dill >=0.3.6"
   ],
   "source_files": [
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests",
    "bindings/python/tests"
   ]
  }
 },
 "win_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "rust_compiler_stub"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "maturin",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "huggingface_hub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "curl",
    "datasets",
    "dill",
    "numpy",
    "pip",
    "pytest",
    "requests"
   ]
  }
 }
}