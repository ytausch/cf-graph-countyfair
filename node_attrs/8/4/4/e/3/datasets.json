{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "bot": {
   "inspection": "hint-all"
  },
  "conda_build": {
   "pkg_format": "2"
  },
  "conda_forge_output_validation": true,
  "github": {
   "branch_name": "main",
   "tooling_branch_name": "main"
  }
 },
 "feedstock_name": "datasets",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "description": "Datasets is a lightweight library providing one-line dataloaders for many\npublic datasets and one liners to download and pre-process any of the number\nof datasets major public datasets provided on the HuggingFace Datasets Hub.\nDatasets are ready to use in a dataloader for training/evaluating a ML model\n(Numpy/Pandas/PyTorch/TensorFlow/JAX). Datasets also provide an API for\nsimple, fast, and reproducible data pre-processing for the above public\ndatasets as well as your own local datasets in CSV/JSON/text.\n",
   "dev_url": "https://github.com/huggingface/datasets",
   "doc_url": "https://huggingface.co/docs/datasets/",
   "home": "https://github.com/huggingface/datasets",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "HuggingFace/Datasets is an open library of NLP datasets."
  },
  "build": {
   "entry_points": [
    "datasets-cli=datasets.commands.datasets_cli:main"
   ],
   "noarch": "python",
   "number": "0",
   "script": "PYTHON -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "oblute",
    "Tata17",
    "thewchan",
    "mxr-conda"
   ]
  },
  "package": {
   "name": "datasets",
   "version": "2.15.0"
  },
  "requirements": {
   "host": [
    "pip",
    "python >=3.8.0"
   ],
   "run": [
    "aiohttp",
    "dill >=0.3.0,<0.3.8",
    "fsspec >=2023.1.0,<=2023.10.0",
    "huggingface_hub >=0.18.0",
    "importlib-metadata",
    "multiprocess",
    "numpy >=1.17",
    "packaging",
    "pandas",
    "pyarrow >=8.0.0",
    "pyarrow-hotfix",
    "python >=3.8.0",
    "python-xxhash",
    "pyyaml >=5.1",
    "requests >=2.19.0",
    "tqdm >=4.62.1"
   ]
  },
  "source": {
   "sha256": "a26d059370bd7503bd60e9337977199a13117a83f72fb61eda7e66f0c4d50b2b",
   "url": "https://pypi.io/packages/source/d/datasets/datasets-2.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "datasets-cli --help"
   ],
   "imports": [
    "datasets",
    "datasets.commands",
    "datasets.features",
    "datasets.filesystems",
    "datasets.formatting",
    "datasets.io",
    "datasets.packaged_modules",
    "datasets.tasks",
    "datasets.utils"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "aiohttp",
    "dill",
    "fsspec",
    "huggingface_hub",
    "importlib-metadata",
    "multiprocess",
    "numpy",
    "packaging",
    "pandas",
    "pyarrow",
    "pyarrow-hotfix",
    "python",
    "python-xxhash",
    "pyyaml",
    "requests",
    "tqdm"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "description": "Datasets is a lightweight library providing one-line dataloaders for many\npublic datasets and one liners to download and pre-process any of the number\nof datasets major public datasets provided on the HuggingFace Datasets Hub.\nDatasets are ready to use in a dataloader for training/evaluating a ML model\n(Numpy/Pandas/PyTorch/TensorFlow/JAX). Datasets also provide an API for\nsimple, fast, and reproducible data pre-processing for the above public\ndatasets as well as your own local datasets in CSV/JSON/text.\n",
   "dev_url": "https://github.com/huggingface/datasets",
   "doc_url": "https://huggingface.co/docs/datasets/",
   "home": "https://github.com/huggingface/datasets",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "HuggingFace/Datasets is an open library of NLP datasets."
  },
  "build": {
   "entry_points": [
    "datasets-cli=datasets.commands.datasets_cli:main"
   ],
   "noarch": "python",
   "number": "0",
   "script": "PYTHON -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "oblute",
    "Tata17",
    "thewchan",
    "mxr-conda"
   ]
  },
  "package": {
   "name": "datasets",
   "version": "2.15.0"
  },
  "requirements": {
   "host": [
    "pip",
    "python >=3.8.0"
   ],
   "run": [
    "aiohttp",
    "dill >=0.3.0,<0.3.8",
    "fsspec >=2023.1.0,<=2023.10.0",
    "huggingface_hub >=0.18.0",
    "importlib-metadata",
    "multiprocess",
    "numpy >=1.17",
    "packaging",
    "pandas",
    "pyarrow >=8.0.0",
    "pyarrow-hotfix",
    "python >=3.8.0",
    "python-xxhash",
    "pyyaml >=5.1",
    "requests >=2.19.0",
    "tqdm >=4.62.1"
   ]
  },
  "source": {
   "sha256": "a26d059370bd7503bd60e9337977199a13117a83f72fb61eda7e66f0c4d50b2b",
   "url": "https://pypi.io/packages/source/d/datasets/datasets-2.15.0.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "datasets-cli --help"
   ],
   "imports": [
    "datasets",
    "datasets.commands",
    "datasets.features",
    "datasets.filesystems",
    "datasets.formatting",
    "datasets.io",
    "datasets.packaged_modules",
    "datasets.tasks",
    "datasets.utils"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "name": "datasets",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "datasets"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/datasets.json"
 },
 "raw_meta_yaml": "{% set name = \"datasets\" %}\n{% set version = \"2.15.0\" %}\n\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: a26d059370bd7503bd60e9337977199a13117a83f72fb61eda7e66f0c4d50b2b\n\nbuild:\n  noarch: python\n  number: 0\n  entry_points:\n    - datasets-cli=datasets.commands.datasets_cli:main\n  script: {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  host:\n    - pip\n    - python >=3.8.0\n  run:\n    - aiohttp\n    - dill >=0.3.0,<0.3.8\n    - fsspec >=2023.1.0,<=2023.10.0\n    - huggingface_hub >=0.18.0\n    - importlib-metadata\n    - multiprocess\n    - numpy >=1.17\n    - packaging\n    - pandas\n    - pyarrow >=8.0.0\n    - pyarrow-hotfix\n    - python >=3.8.0\n    - python-xxhash\n    - pyyaml >=5.1\n    - requests >=2.19.0\n    - tqdm >=4.62.1\n\ntest:\n  imports:\n    - datasets\n    - datasets.commands\n    - datasets.features\n    - datasets.filesystems\n    - datasets.formatting\n    - datasets.io\n    - datasets.packaged_modules\n    - datasets.tasks\n    - datasets.utils\n  commands:\n    - pip check\n    - datasets-cli --help\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/huggingface/datasets\n  license: Apache-2.0\n  license_family: Apache\n  license_file: LICENSE\n  summary: HuggingFace/Datasets is an open library of NLP datasets.\n  description: |\n    Datasets is a lightweight library providing one-line dataloaders for many\n    public datasets and one liners to download and pre-process any of the number\n    of datasets major public datasets provided on the HuggingFace Datasets Hub.\n    Datasets are ready to use in a dataloader for training/evaluating a ML model\n    (Numpy/Pandas/PyTorch/TensorFlow/JAX). Datasets also provide an API for\n    simple, fast, and reproducible data pre-processing for the above public\n    datasets as well as your own local datasets in CSV/JSON/text.\n  doc_url: https://huggingface.co/docs/datasets/\n  dev_url: https://github.com/huggingface/datasets\n\nextra:\n  recipe-maintainers:\n    - oblute\n    - Tata17\n    - thewchan\n    - mxr-conda\n",
 "req": {
  "__set__": true,
  "elements": [
   "aiohttp",
   "dill",
   "fsspec",
   "huggingface_hub",
   "importlib-metadata",
   "multiprocess",
   "numpy",
   "packaging",
   "pandas",
   "pip",
   "pyarrow",
   "pyarrow-hotfix",
   "python",
   "python-xxhash",
   "pyyaml",
   "requests",
   "tqdm"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "aiohttp",
    "dill",
    "fsspec",
    "huggingface_hub",
    "importlib-metadata",
    "multiprocess",
    "numpy",
    "packaging",
    "pandas",
    "pyarrow",
    "pyarrow-hotfix",
    "python",
    "python-xxhash",
    "pyyaml",
    "requests",
    "tqdm"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python >=3.8.0"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "aiohttp",
    "dill >=0.3.0,<0.3.8",
    "fsspec >=2023.1.0,<=2023.10.0",
    "huggingface_hub >=0.18.0",
    "importlib-metadata",
    "multiprocess",
    "numpy >=1.17",
    "packaging",
    "pandas",
    "pyarrow >=8.0.0",
    "pyarrow-hotfix",
    "python >=3.8.0",
    "python-xxhash",
    "pyyaml >=5.1",
    "requests >=2.19.0",
    "tqdm >=4.62.1"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "url": "https://pypi.io/packages/source/d/datasets/datasets-2.15.0.tar.gz",
 "version": "2.15.0",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/datasets.json"
 }
}