{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {},
 "feedstock_name": "gptcache",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "home": "https://github.com/zilliztech/GPTCache",
   "license": "MIT",
   "license_file": "LICENSE",
   "summary": "GPTCache, a powerful caching library that can be used to speed up and lower the cost of chat applications that rely on the LLM service. GPTCache works as a memcache for AIGC applications, similar to how Redis works for traditional applications."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/usr/share/miniconda3/envs/cf-scripts/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_/bin/python -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "YYYasin19"
   ]
  },
  "package": {
   "name": "gptcache",
   "version": "0.1.25"
  },
  "requirements": {
   "host": [
    "python >=3.8",
    "pip"
   ],
   "run": [
    "python >=3.8",
    "openai",
    "numpy",
    "cachetools"
   ]
  },
  "source": {
   "sha256": "9634fbbbc2151d12376cd53667682b912ce46812d793331d2e2d46a8a127ed9a",
   "url": "https://pypi.io/packages/source/g/gptcache/gptcache-0.1.25.tar.gz"
  },
  "test": {
   "commands": [
    "pip check"
   ],
   "imports": [
    "gptcache"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cachetools",
    "numpy",
    "openai",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "home": "https://github.com/zilliztech/GPTCache",
   "license": "MIT",
   "license_file": "LICENSE",
   "summary": "GPTCache, a powerful caching library that can be used to speed up and lower the cost of chat applications that rely on the LLM service. GPTCache works as a memcache for AIGC applications, similar to how Redis works for traditional applications."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/usr/share/miniconda3/envs/cf-scripts/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_/bin/python -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "YYYasin19"
   ]
  },
  "package": {
   "name": "gptcache",
   "version": "0.1.25"
  },
  "requirements": {
   "host": [
    "python >=3.8",
    "pip"
   ],
   "run": [
    "python >=3.8",
    "openai",
    "numpy",
    "cachetools"
   ]
  },
  "source": {
   "sha256": "9634fbbbc2151d12376cd53667682b912ce46812d793331d2e2d46a8a127ed9a",
   "url": "https://pypi.io/packages/source/g/gptcache/gptcache-0.1.25.tar.gz"
  },
  "test": {
   "commands": [
    "pip check"
   ],
   "imports": [
    "gptcache"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "name": "gptcache",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "gptcache"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/gptcache.json"
 },
 "raw_meta_yaml": "{% set name = \"gptcache\" %}\n{% set version = \"0.1.25\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/gptcache-{{ version }}.tar.gz\n  sha256: 9634fbbbc2151d12376cd53667682b912ce46812d793331d2e2d46a8a127ed9a\n\nbuild:\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n  number: 0\n\nrequirements:\n  host:\n    - python >=3.8\n    - pip\n  run:\n    - python >=3.8\n    - openai\n    - numpy\n    - cachetools\n\ntest:\n  imports:\n    - gptcache\n  commands:\n    - pip check\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/zilliztech/GPTCache\n  summary: GPTCache, a powerful caching library that can be used to speed up and lower the cost of chat applications that rely on the LLM service. GPTCache works as a memcache for AIGC applications, similar to how Redis works for traditional applications.\n  license: MIT\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - YYYasin19\n",
 "req": {
  "__set__": true,
  "elements": [
   "cachetools",
   "numpy",
   "openai",
   "pip",
   "python"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cachetools",
    "numpy",
    "openai",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python >=3.8"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cachetools",
    "numpy",
    "openai",
    "python >=3.8"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "url": "https://pypi.io/packages/source/g/gptcache/gptcache-0.1.25.tar.gz",
 "version": "0.1.25",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/gptcache.json"
 }
}