{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "bot": {
   "automerge": true
  },
  "conda_build": {
   "error_overlinking": true
  },
  "conda_forge_output_validation": true,
  "github": {
   "branch_name": "main",
   "tooling_branch_name": "main"
  }
 },
 "feedstock_name": "r-robotstxt",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "home": "https://docs.ropensci.org/robotstxt/, https://github.com/ropensci/robotstxt",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": [
    "/lib/R/share/licenses/MIT",
    "LICENSE",
    "/lib/R/share/licenses/MIT",
    "LICENSE"
   ],
   "summary": "Provides functions to download and parse 'robots.txt' files. Ultimately the package makes it easy to check if bots (spiders, crawler, scrapers, ...) are allowed to access specific resources on a domain."
  },
  "build": {
   "noarch": "generic",
   "number": "0",
   "rpaths": [
    "lib/R/lib/",
    "lib/",
    "lib/R/lib/",
    "lib/"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "conda-forge/r",
    "conda-forge/r"
   ]
  },
  "package": {
   "name": "r-robotstxt",
   "version": "0.7.13"
  },
  "requirements": {
   "build": [],
   "host": [
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0",
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0"
   ],
   "run": [
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0",
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0"
   ]
  },
  "source": {
   "sha256": "872a37a548f1cdaf13ca6d01d45498431c601c157fbc1aa412acbce32053baf4",
   "url": [
    "https://cran.r-project.org/src/contrib/robotstxt_0.7.13.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.13.tar.gz",
    "https://cran.r-project.org/src/contrib/robotstxt_0.7.13.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.13.tar.gz"
   ]
  },
  "test": {
   "commands": [
    "$R -e \"library('robotstxt')\"",
    "$R -e \"library('robotstxt')\""
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "r-base",
    "r-future",
    "r-future.apply",
    "r-httr",
    "r-magrittr",
    "r-spiderbar",
    "r-stringr"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "r-base",
    "r-future",
    "r-future.apply",
    "r-httr",
    "r-magrittr",
    "r-spiderbar",
    "r-stringr"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "meta_yaml": {
  "about": {
   "home": "https://docs.ropensci.org/robotstxt/, https://github.com/ropensci/robotstxt",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": [
    "/lib/R/share/licenses/MIT",
    "LICENSE",
    "/lib/R/share/licenses/MIT",
    "LICENSE"
   ],
   "summary": "Provides functions to download and parse 'robots.txt' files. Ultimately the package makes it easy to check if bots (spiders, crawler, scrapers, ...) are allowed to access specific resources on a domain."
  },
  "build": {
   "noarch": "generic",
   "number": "0",
   "rpaths": [
    "lib/R/lib/",
    "lib/",
    "lib/R/lib/",
    "lib/"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "conda-forge/r",
    "conda-forge/r"
   ]
  },
  "package": {
   "name": "r-robotstxt",
   "version": "0.7.13"
  },
  "requirements": {
   "build": [],
   "host": [
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0",
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0"
   ],
   "run": [
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0",
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0"
   ]
  },
  "source": {
   "sha256": "872a37a548f1cdaf13ca6d01d45498431c601c157fbc1aa412acbce32053baf4",
   "url": [
    "https://cran.r-project.org/src/contrib/robotstxt_0.7.13.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.13.tar.gz",
    "https://cran.r-project.org/src/contrib/robotstxt_0.7.13.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.13.tar.gz"
   ]
  },
  "test": {
   "commands": [
    "$R -e \"library('robotstxt')\"",
    "$R -e \"library('robotstxt')\""
   ]
  }
 },
 "name": "r-robotstxt",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "r-robotstxt"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/r-robotstxt.json"
 },
 "raw_meta_yaml": "{% set version = '0.7.13' %}\n{% set posix = 'm2-' if win else '' %}\n{% set native = 'm2w64-' if win else '' %}\n\npackage:\n  name: r-robotstxt\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  url:\n    - {{ cran_mirror }}/src/contrib/robotstxt_{{ version }}.tar.gz\n    - {{ cran_mirror }}/src/contrib/Archive/robotstxt/robotstxt_{{ version }}.tar.gz\n  sha256: 872a37a548f1cdaf13ca6d01d45498431c601c157fbc1aa412acbce32053baf4\n\nbuild:\n  merge_build_host: True  # [win]\n  number: 0\n  noarch: generic\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - {{ posix }}zip               # [win]\n    - cross-r-base {{ r_base }}    # [build_platform != target_platform]\n  host:\n    - r-base\n    - r-future >=1.6.2\n    - r-future.apply >=1.0.0\n    - r-httr >=1.0.0\n    - r-magrittr\n    - r-spiderbar >=0.2.0\n    - r-stringr >=1.0.0\n  run:\n    - r-base\n    - r-future >=1.6.2\n    - r-future.apply >=1.0.0\n    - r-httr >=1.0.0\n    - r-magrittr\n    - r-spiderbar >=0.2.0\n    - r-stringr >=1.0.0\n\ntest:\n  commands:\n    - $R -e \"library('robotstxt')\"           # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('robotstxt')\\\"\"  # [win]\n\nabout:\n  home: https://docs.ropensci.org/robotstxt/, https://github.com/ropensci/robotstxt\n  license: MIT\n  summary: Provides functions to download and parse 'robots.txt' files. Ultimately the package\n    makes it easy to check if bots (spiders, crawler, scrapers, ...) are allowed to\n    access specific resources on a domain.\n  license_family: MIT\n  license_file:\n    - '{{ environ[\"PREFIX\"] }}/lib/R/share/licenses/MIT'\n    - LICENSE\n\nextra:\n  recipe-maintainers:\n    - conda-forge/r\n",
 "req": {
  "__set__": true,
  "elements": [
   "r-base",
   "r-future",
   "r-future.apply",
   "r-httr",
   "r-magrittr",
   "r-spiderbar",
   "r-stringr"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "r-base",
    "r-future",
    "r-future.apply",
    "r-httr",
    "r-magrittr",
    "r-spiderbar",
    "r-stringr"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "r-base",
    "r-future",
    "r-future.apply",
    "r-httr",
    "r-magrittr",
    "r-spiderbar",
    "r-stringr"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "r-base",
    "r-future >=1.6.2",
    "r-future.apply >=1.0.0",
    "r-httr >=1.0.0",
    "r-magrittr",
    "r-spiderbar >=0.2.0",
    "r-stringr >=1.0.0"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "url": [
  "https://cran.r-project.org/src/contrib/robotstxt_0.7.13.tar.gz",
  "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.13.tar.gz",
  "https://cran.r-project.org/src/contrib/robotstxt_0.7.13.tar.gz",
  "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.13.tar.gz"
 ],
 "version": "0.7.13",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/r-robotstxt.json"
 }
}