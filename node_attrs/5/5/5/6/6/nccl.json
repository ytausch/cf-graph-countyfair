{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "azure": {
   "free_disk_space": true
  },
  "bot": {
   "automerge": true
  },
  "build_platform": {
   "linux_aarch64": "linux_64",
   "linux_ppc64le": "linux_64"
  },
  "conda_build": {
   "pkg_format": "2"
  },
  "conda_forge_output_validation": true,
  "github": {
   "branch_name": "main",
   "tooling_branch_name": "main"
  },
  "os_version": {
   "linux_64": "cos6",
   "linux_aarch64": "cos7",
   "linux_ppc64le": "cos7"
  },
  "provider": {
   "linux_aarch64": "azure",
   "linux_ppc64le": "azure"
  },
  "test": "native_and_emulated"
 },
 "feedstock_name": "nccl",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url": "https://github.com/NVIDIA/nccl",
   "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home": "https://developer.nvidia.com/nccl",
   "license": "BSD-3-Clause",
   "license_family": "BSD",
   "license_file": "LICENSE.txt",
   "summary": "Optimized primitives for collective multi-GPU communication"
  },
  "build": {
   "ignore_run_exports_from": null,
   "number": "0",
   "run_exports": [
    "nccl",
    "nccl",
    "nccl"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang"
   ]
  },
  "package": {
   "name": "nccl",
   "version": "2.19.4.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ],
   "host": [
    "cuda-version =11.2",
    "cuda-version =11.8"
   ],
   "run": [
    "cuda-version",
    "cuda-version"
   ]
  },
  "source": {
   "patches": [
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch"
   ],
   "sha256": "a3948ade5d83a036dde7ca6d0fee3a960a4de5c7915a9be903a28a747b5babc4",
   "url": "https://github.com/NVIDIA/nccl/archive/v2.19.4-1.tar.gz"
  },
  "test": {
   "commands": [
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-version"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cuda-version"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "linux_aarch64_meta_yaml": {
  "about": {
   "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url": "https://github.com/NVIDIA/nccl",
   "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home": "https://developer.nvidia.com/nccl",
   "license": "BSD-3-Clause",
   "license_family": "BSD",
   "license_file": "LICENSE.txt",
   "summary": "Optimized primitives for collective multi-GPU communication"
  },
  "build": {
   "ignore_run_exports_from": null,
   "number": "0",
   "run_exports": [
    "nccl",
    "nccl",
    "nccl"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang"
   ]
  },
  "package": {
   "name": "nccl",
   "version": "2.19.4.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ],
   "host": [
    "cuda-version =11.2",
    "cuda-version =11.8"
   ],
   "run": [
    "cuda-version",
    "cuda-version"
   ]
  },
  "source": {
   "patches": [
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch"
   ],
   "sha256": "a3948ade5d83a036dde7ca6d0fee3a960a4de5c7915a9be903a28a747b5babc4",
   "url": "https://github.com/NVIDIA/nccl/archive/v2.19.4-1.tar.gz"
  },
  "test": {
   "commands": [
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "linux_aarch64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-version"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cuda-version"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "linux_ppc64le_meta_yaml": {
  "about": {
   "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url": "https://github.com/NVIDIA/nccl",
   "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home": "https://developer.nvidia.com/nccl",
   "license": "BSD-3-Clause",
   "license_family": "BSD",
   "license_file": "LICENSE.txt",
   "summary": "Optimized primitives for collective multi-GPU communication"
  },
  "build": {
   "ignore_run_exports_from": null,
   "number": "0",
   "run_exports": [
    "nccl",
    "nccl",
    "nccl"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang"
   ]
  },
  "package": {
   "name": "nccl",
   "version": "2.19.4.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ],
   "host": [
    "cuda-version =11.2",
    "cuda-version =11.8"
   ],
   "run": [
    "cuda-version",
    "cuda-version"
   ]
  },
  "source": {
   "patches": [
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch"
   ],
   "sha256": "a3948ade5d83a036dde7ca6d0fee3a960a4de5c7915a9be903a28a747b5babc4",
   "url": "https://github.com/NVIDIA/nccl/archive/v2.19.4-1.tar.gz"
  },
  "test": {
   "commands": [
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "linux_ppc64le_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-version"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cuda-version"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "meta_yaml": {
  "about": {
   "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url": "https://github.com/NVIDIA/nccl",
   "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home": "https://developer.nvidia.com/nccl",
   "license": "BSD-3-Clause",
   "license_family": "BSD",
   "license_file": "LICENSE.txt",
   "summary": "Optimized primitives for collective multi-GPU communication"
  },
  "build": {
   "ignore_run_exports_from": null,
   "number": "0",
   "run_exports": [
    "nccl",
    "nccl",
    "nccl",
    "nccl",
    "nccl",
    "nccl",
    "nccl",
    "nccl",
    "nccl"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang",
    "jakirkham",
    "leofang"
   ]
  },
  "package": {
   "name": "nccl",
   "version": "2.19.4.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ],
   "host": [
    "cuda-version =11.2",
    "cuda-version =11.8",
    "cuda-version =11.2",
    "cuda-version =11.8",
    "cuda-version =11.2",
    "cuda-version =11.8"
   ],
   "run": [
    "cuda-version",
    "cuda-version",
    "cuda-version",
    "cuda-version",
    "cuda-version",
    "cuda-version"
   ]
  },
  "source": {
   "patches": [
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch",
    "0001-Allow-custom-NVCC-path.patch"
   ],
   "sha256": "a3948ade5d83a036dde7ca6d0fee3a960a4de5c7915a9be903a28a747b5babc4",
   "url": "https://github.com/NVIDIA/nccl/archive/v2.19.4-1.tar.gz"
  },
  "test": {
   "commands": [
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test ! -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "name": "nccl",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "nccl"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64",
  "linux_aarch64",
  "linux_ppc64le"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/nccl.json"
 },
 "raw_meta_yaml": "{% set name = \"nccl\" %}\n{% set version = \"2.19.4-1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version|replace(\"-\", \".\") }}\n\nsource:\n  url: https://github.com/NVIDIA/nccl/archive/v{{ version }}.tar.gz\n  sha256: a3948ade5d83a036dde7ca6d0fee3a960a4de5c7915a9be903a28a747b5babc4\n  patches:\n    # Upstreaming w/PR: https://github.com/NVIDIA/nccl/pull/854\n    - 0001-Allow-custom-NVCC-path.patch\n\nbuild:\n  number: 0\n  skip: true  # [(not linux) or cuda_compiler_version in (undefined, \"None\", \"10.2\")]\n  ignore_run_exports_from:\n    # Ignore `cudatoolkit` dependency in CUDA 11 builds\n    - {{ compiler(\"cuda\") }}  # [(cuda_compiler_version or \"\").startswith(\"11\")]\n  run_exports:\n    # xref: https://github.com/NVIDIA/nccl/issues/218\n    - {{ pin_subpackage(name, max_pin=\"x\") }}\n\nrequirements:\n  build:\n    - {{ compiler(\"c\") }}\n    - {{ compiler(\"cxx\") }}\n    - {{ compiler(\"cuda\") }}\n    - make\n  host:\n    - cuda-version ={{ cuda_compiler_version }}          # [(cuda_compiler_version or \"\").startswith(\"11\")]\n  run:\n    - {{ pin_compatible(\"cuda-version\", max_pin=\"x\") }}  # [(cuda_compiler_version or \"\").startswith(\"11\")]\n\ntest:\n  commands:\n    - test -f \"${PREFIX}/include/nccl.h\"\n    - test -f \"${PREFIX}/lib/libnccl.so\"\n    - test ! -f \"${PREFIX}/lib/libnccl_static.a\"\n\nabout:\n  home: https://developer.nvidia.com/nccl\n  license: BSD-3-Clause\n  license_family: BSD\n  license_file: LICENSE.txt\n  summary: Optimized primitives for collective multi-GPU communication\n\n  description: |\n    The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\n    and multi-node collective communication primitives that are performance\n    optimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\n    all-reduce, broadcast, reduce, reduce-scatter, that are optimized to\n    achieve high bandwidth over PCIe and NVLink high-speed interconnect.\n\n  doc_url: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html\n  dev_url: https://github.com/NVIDIA/nccl\n\nextra:\n  recipe-maintainers:\n    - jakirkham\n    - leofang\n",
 "req": {
  "__set__": true,
  "elements": [
   "c_compiler_stub",
   "cuda-version",
   "cuda_compiler_stub",
   "cxx_compiler_stub",
   "make"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda-version",
    "cuda_compiler_stub",
    "cxx_compiler_stub"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda-version",
    "cuda_compiler_stub",
    "cxx_compiler_stub"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-version =11.2",
    "cuda-version =11.8"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cuda-version"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "url": "https://github.com/NVIDIA/nccl/archive/v2.19.4-1.tar.gz",
 "version": "2.19.4.1",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/nccl.json"
 }
}